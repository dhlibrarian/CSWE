{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases, TfidfModel, CoherenceModel, LdaMulticore, HdpModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lemmatizer and define helper functions with NLTK\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize Lemmatizer and define helper functions\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    tag = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Remove special characters while keeping possessive apostrophes\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9_' ]\", \"\", sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in nltk.pos_tag(tokens)]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def preprocess_keywords(data):\n",
    "    keywords = data['terms'].str.split().tolist()\n",
    "    keywords = [[word for word in keyword if word not in gensim.parsing.preprocessing.STOPWORDS] for keyword in keywords]\n",
    "    bigram_transformer = Phrases(keywords, min_count=5, threshold=100)\n",
    "    keywords_bigram = [bigram_transformer[keyword] for keyword in keywords]\n",
    "    trigram_transformer = Phrases(keywords_bigram, min_count=5, threshold=100)\n",
    "    keywords_trigram = [trigram_transformer[keyword] for keyword in keywords_bigram]\n",
    "    return [lemmatize_sentence(' '.join(keyword)) for keyword in keywords_trigram]\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"corrected_data_symspell.csv\")\n",
    "\n",
    "# Preprocess the data to get lemmatized keywords\n",
    "keywords_lemmatized = preprocess_keywords(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9ac7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(keywords_lemmatized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(term) for term in keywords_lemmatized]\n",
    "\n",
    "# TF-IDF Weighting\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de85aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_lda(corpus, dictionary, num_topics, filename, keywords_lemmatized):\n",
    "    lda_model = LdaMulticore(corpus, num_topics=20, id2word=dictionary, passes=100, workers=12)\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, filename + \"_vis.html\")\n",
    "    pyLDAvis.display(vis)\n",
    "    lda_model.save(filename + \"_model\")\n",
    "    topics_lda = lda_model.print_topics(num_words=10)\n",
    "    df_topics_lda = pd.DataFrame(topics_lda, columns=['Topic_ID', 'Keywords'])\n",
    "    df_topics_lda.to_csv(filename + \"_topics.csv\", index=False)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"Coherence for {filename}: {coherence_lda}\")\n",
    "    return lda_model\n",
    "    return vis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_lda(corpus, dictionary, num_topics, filename, keywords_lemmatized):\n",
    "    lda_model = LdaMulticore(corpus, num_topics=15, id2word=dictionary, passes=100, workers=12)\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, filename + \"_vis.html\")\n",
    "    pyLDAvis.display(vis)\n",
    "    lda_model.save(filename + \"_model\")\n",
    "    topics_lda = lda_model.print_topics(num_words=20)\n",
    "    df_topics_lda = pd.DataFrame(topics_lda, columns=['Topic_ID', 'Keywords'])\n",
    "    df_topics_lda.to_csv(filename + \"_topics.csv\", index=False)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"Coherence for {filename}: {coherence_lda}\")\n",
    "    return vis \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f610e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_regular = train_and_save_lda(corpus, dictionary, 7, 'lda_regular', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea51ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis_tfidf = train_and_save_lda(corpus_tfidf, dictionary, 7, 'lda_tfidf', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a61083",
   "metadata": {},
   "source": [
    "## Using HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a2c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_hdp(corpus, dictionary, filename, keywords_lemmatized):\n",
    "    hdp_model = HdpModel(corpus, dictionary)\n",
    "    # Convert HDP topics to LDA format for visualization (HDP provides dynamic topic count)\n",
    "    lda_model = hdp_model.suggested_lda_model()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, filename + \"_hdp_vis.html\")\n",
    "    pyLDAvis.display(vis)\n",
    "    lda_model.save(filename + \"_hdp_model\")\n",
    "    topics_hdp = lda_model.print_topics(num_words=10)\n",
    "    df_topics_hdp = pd.DataFrame(topics_hdp, columns=['Topic_ID', 'Keywords'])\n",
    "    df_topics_hdp.to_csv(filename + \"_hdp_topics.csv\", index=False)\n",
    "    coherence_model_hdp = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_hdp = coherence_model_hdp.get_coherence()\n",
    "    print(f\"Coherence for {filename} HDP: {coherence_hdp}\")\n",
    "    return vis  # Return the vis object for display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935d6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hdp_regular = train_and_save_hdp(corpus, dictionary, 'hdp_regular', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_hdp_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4387ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hdp_tfidf = train_and_save_hdp(corpus_tfidf, dictionary, 'hdp_tfidf', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_hdp_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf1b3c9",
   "metadata": {},
   "source": [
    "# Adjusting ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import Phrases, TfidfModel, CoherenceModel, LdaMulticore, HdpModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc71eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Lemmatizer and define helper functions. Make sure variables and functions up to here are reset.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    tag = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Remove special characters while keeping possessive apostrophes\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9_' ]\", \"\", sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in nltk.pos_tag(tokens)]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def preprocess_keywords(data):\n",
    "    keywords = data['terms'].str.split().tolist()\n",
    "    keywords = [[word for word in keyword if word not in gensim.parsing.preprocessing.STOPWORDS] for keyword in keywords]\n",
    "    bigram_transformer = Phrases(keywords, min_count=5, threshold=100)\n",
    "    keywords_bigram = [bigram_transformer[keyword] for keyword in keywords]\n",
    "    trigram_transformer = Phrases(keywords_bigram, min_count=5, threshold=100)\n",
    "    keywords_trigram = [trigram_transformer[keyword] for keyword in keywords_bigram]\n",
    "    return [lemmatize_sentence(' '.join(keyword)) for keyword in keywords_trigram]\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"corrected_data_symspell.csv\")\n",
    "\n",
    "# Preprocess the data to get lemmatized keywords\n",
    "keywords_lemmatized = preprocess_keywords(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65cb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(keywords_lemmatized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(term) for term in keywords_lemmatized]\n",
    "\n",
    "# TF-IDF Weighting\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb150a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_lda(corpus, dictionary, num_topics, filename, keywords_lemmatized):\n",
    "    lda_model = LdaMulticore(corpus, num_topics=20, id2word=dictionary, passes=100, workers=12)\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, filename + \"_vis.html\")\n",
    "    pyLDAvis.display(vis)\n",
    "    lda_model.save(filename + \"_model\")\n",
    "    topics_lda = lda_model.print_topics(num_words=10)\n",
    "    df_topics_lda = pd.DataFrame(topics_lda, columns=['Topic_ID', 'Keywords'])\n",
    "    df_topics_lda.to_csv(filename + \"_topics.csv\", index=False)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    print(f\"Coherence for {filename}: {coherence_lda}\")\n",
    "    return lda_model\n",
    "    return vis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_regular = train_and_save_lda(corpus, dictionary, 7, 'lda_regular', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vis_tfidf = train_and_save_lda(corpus_tfidf, dictionary, 7, 'lda_tfidf', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa1b02",
   "metadata": {},
   "source": [
    "### With HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e77619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_hdp(corpus, dictionary, filename, keywords_lemmatized):\n",
    "    hdp_model = HdpModel(corpus, dictionary)\n",
    "    # Convert HDP topics to LDA format for visualization (HDP provides dynamic topic count)\n",
    "    lda_model = hdp_model.suggested_lda_model()\n",
    "    vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "    pyLDAvis.save_html(vis, filename + \"_hdp_vis.html\")\n",
    "    pyLDAvis.display(vis)\n",
    "    lda_model.save(filename + \"_hdp_model\")\n",
    "    topics_hdp = lda_model.print_topics(num_words=10)\n",
    "    df_topics_hdp = pd.DataFrame(topics_hdp, columns=['Topic_ID', 'Keywords'])\n",
    "    df_topics_hdp.to_csv(filename + \"_hdp_topics.csv\", index=False)\n",
    "    coherence_model_hdp = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_hdp = coherence_model_hdp.get_coherence()\n",
    "    print(f\"Coherence for {filename} HDP: {coherence_hdp}\")\n",
    "    return vis  # Return the vis object for display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988c4125",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hdp_regular = train_and_save_hdp(corpus, dictionary, 'hdp_regular', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_hdp_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9484e75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_hdp_tfidf = train_and_save_hdp(corpus_tfidf, dictionary, 'hdp_tfidf', keywords_lemmatized)\n",
    "pyLDAvis.display(vis_hdp_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa19e6",
   "metadata": {},
   "source": [
    "# Iterative Training for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be8bd2-ad4c-4235-a586-6ff2af1accbd",
   "metadata": {},
   "source": [
    "## quick training to finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adc5885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import TfidfModel, CoherenceModel, LdaMulticore, HdpModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "import pyLDAvis.gensim_models\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "pyLDAvis.enable_notebook()\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c7a0b0c-82cd-46af-ac04-ac8c6d5c1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting term detection to prioritize bi-grams and tri-grams\n",
    "# Cell after was the initial attempt\n",
    "\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# parts of speech tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    tag = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "# lemmatizing tokens and cleaning data\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Remove special characters while keeping possessive apostrophes\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9_' ]\", \"\", sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in nltk.pos_tag(tokens)]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# In case lemmatization and stopwords missed things plus more weight on bi-grams and tri-grams\n",
    "def preprocess_keywords(data):\n",
    "    keywords = data['terms'].str.lower().str.split().tolist()  # Convert to lowercase\n",
    "    keywords = [[word for word in keyword if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 2 and word.isalnum()] for keyword in keywords]\n",
    "    \n",
    "    # Better bi-grams\n",
    "    bigram_transformer = Phrases(keywords, min_count=3, threshold=50)  \n",
    "    keywords_bigram = [bigram_transformer[keyword] for keyword in keywords]\n",
    "    \n",
    "    # Better tri-gram\n",
    "    trigram_transformer = Phrases(keywords_bigram, min_count=3, threshold=50)  \n",
    "    keywords_trigram = [trigram_transformer[keyword] for keyword in keywords_bigram]\n",
    "    \n",
    "    return [lemmatize_sentence(' '.join(keyword)) for keyword in keywords_trigram]\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"corrected_data_symspell.csv\")\n",
    "\n",
    "\n",
    "# Preprocess the data to get lemmatized keywords\n",
    "keywords_lemmatized = preprocess_keywords(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8dc4c94-1e44-4ebb-a078-a89e635eabb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(keywords_lemmatized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(term) for term in keywords_lemmatized]\n",
    "\n",
    "# TF-IDF Weighting\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a132494-dd4d-4ff8-b084-49e27490ad9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 681 ms, total: 2.67 s\n",
      "Wall time: 8.23 s\n",
      "CPU times: user 2.18 s, sys: 590 ms, total: 2.77 s\n",
      "Wall time: 8.63 s\n",
      "CPU times: user 1.5 s, sys: 395 ms, total: 1.9 s\n",
      "Wall time: 7.31 s\n",
      "CPU times: user 2.29 s, sys: 727 ms, total: 3.02 s\n",
      "Wall time: 7.74 s\n",
      "CPU times: user 1.97 s, sys: 609 ms, total: 2.58 s\n",
      "Wall time: 7.1 s\n",
      "CPU times: user 2.52 s, sys: 926 ms, total: 3.44 s\n",
      "Wall time: 9.18 s\n",
      "CPU times: user 1.98 s, sys: 636 ms, total: 2.62 s\n",
      "Wall time: 9.3 s\n",
      "CPU times: user 2.46 s, sys: 749 ms, total: 3.21 s\n",
      "Wall time: 9.37 s\n",
      "CPU times: user 1.78 s, sys: 632 ms, total: 2.42 s\n",
      "Wall time: 6.57 s\n",
      "CPU times: user 2.39 s, sys: 732 ms, total: 3.12 s\n",
      "Wall time: 8.85 s\n",
      "CPU times: user 1.85 s, sys: 578 ms, total: 2.43 s\n",
      "Wall time: 7.08 s\n",
      "CPU times: user 2.36 s, sys: 674 ms, total: 3.04 s\n",
      "Wall time: 9.51 s\n",
      "CPU times: user 1.85 s, sys: 588 ms, total: 2.44 s\n",
      "Wall time: 7.89 s\n",
      "CPU times: user 2.27 s, sys: 731 ms, total: 3 s\n",
      "Wall time: 8.03 s\n",
      "CPU times: user 1.66 s, sys: 579 ms, total: 2.24 s\n",
      "Wall time: 6.22 s\n",
      "CPU times: user 2.46 s, sys: 732 ms, total: 3.2 s\n",
      "Wall time: 9.23 s\n",
      "CPU times: user 1.77 s, sys: 592 ms, total: 2.36 s\n",
      "Wall time: 7.06 s\n",
      "CPU times: user 2.08 s, sys: 596 ms, total: 2.68 s\n",
      "Wall time: 8.3 s\n",
      "CPU times: user 1.39 s, sys: 381 ms, total: 1.78 s\n",
      "Wall time: 6.89 s\n",
      "CPU times: user 2.28 s, sys: 733 ms, total: 3.01 s\n",
      "Wall time: 7.58 s\n",
      "CPU times: user 1.65 s, sys: 544 ms, total: 2.19 s\n",
      "Wall time: 6.98 s\n",
      "CPU times: user 2.31 s, sys: 689 ms, total: 3 s\n",
      "Wall time: 8.6 s\n",
      "CPU times: user 1.45 s, sys: 473 ms, total: 1.92 s\n",
      "Wall time: 7.05 s\n",
      "CPU times: user 2.24 s, sys: 728 ms, total: 2.97 s\n",
      "Wall time: 7.99 s\n",
      "CPU times: user 1.8 s, sys: 510 ms, total: 2.31 s\n",
      "Wall time: 7.09 s\n",
      "CPU times: user 2.33 s, sys: 695 ms, total: 3.02 s\n",
      "Wall time: 7.46 s\n",
      "CPU times: user 1.96 s, sys: 551 ms, total: 2.52 s\n",
      "Wall time: 7.2 s\n",
      "CPU times: user 2.63 s, sys: 741 ms, total: 3.37 s\n",
      "Wall time: 8.55 s\n",
      "CPU times: user 1.87 s, sys: 494 ms, total: 2.36 s\n",
      "Wall time: 7.3 s\n",
      "CPU times: user 2.44 s, sys: 733 ms, total: 3.17 s\n",
      "Wall time: 7.78 s\n",
      "CPU times: user 2.17 s, sys: 624 ms, total: 2.79 s\n",
      "Wall time: 7.83 s\n",
      "CPU times: user 2.53 s, sys: 688 ms, total: 3.22 s\n",
      "Wall time: 9.32 s\n",
      "CPU times: user 2.04 s, sys: 550 ms, total: 2.59 s\n",
      "Wall time: 8.16 s\n",
      "CPU times: user 2.43 s, sys: 626 ms, total: 3.05 s\n",
      "Wall time: 9.3 s\n",
      "CPU times: user 2.12 s, sys: 682 ms, total: 2.8 s\n",
      "Wall time: 6.88 s\n",
      "CPU times: user 2.52 s, sys: 682 ms, total: 3.2 s\n",
      "Wall time: 8.3 s\n",
      "CPU times: user 1.92 s, sys: 533 ms, total: 2.45 s\n",
      "Wall time: 7.76 s\n",
      "CPU times: user 2.59 s, sys: 763 ms, total: 3.35 s\n",
      "Wall time: 8.45 s\n",
      "CPU times: user 2.07 s, sys: 717 ms, total: 2.79 s\n",
      "Wall time: 7.71 s\n",
      "CPU times: user 2.44 s, sys: 652 ms, total: 3.09 s\n",
      "Wall time: 10.4 s\n",
      "CPU times: user 1.81 s, sys: 501 ms, total: 2.31 s\n",
      "Wall time: 7.09 s\n",
      "CPU times: user 1.99 s, sys: 545 ms, total: 2.54 s\n",
      "Wall time: 6.99 s\n",
      "CPU times: user 2.01 s, sys: 579 ms, total: 2.59 s\n",
      "Wall time: 6.76 s\n",
      "CPU times: user 2.44 s, sys: 613 ms, total: 3.05 s\n",
      "Wall time: 8.79 s\n",
      "CPU times: user 1.9 s, sys: 499 ms, total: 2.4 s\n",
      "Wall time: 7.55 s\n",
      "CPU times: user 2.19 s, sys: 644 ms, total: 2.84 s\n",
      "Wall time: 8.04 s\n",
      "CPU times: user 1.32 s, sys: 372 ms, total: 1.7 s\n",
      "Wall time: 7.11 s\n",
      "CPU times: user 2.04 s, sys: 634 ms, total: 2.67 s\n",
      "Wall time: 7.36 s\n",
      "CPU times: user 2.55 s, sys: 562 ms, total: 3.12 s\n",
      "Wall time: 7.91 s\n",
      "CPU times: user 2.91 s, sys: 636 ms, total: 3.54 s\n",
      "Wall time: 9.85 s\n",
      "CPU times: user 2.47 s, sys: 544 ms, total: 3.01 s\n",
      "Wall time: 8.92 s\n",
      "CPU times: user 2.95 s, sys: 650 ms, total: 3.6 s\n",
      "Wall time: 9.58 s\n",
      "CPU times: user 2.28 s, sys: 660 ms, total: 2.94 s\n",
      "Wall time: 7.66 s\n",
      "CPU times: user 3.01 s, sys: 686 ms, total: 3.7 s\n",
      "Wall time: 10.3 s\n",
      "CPU times: user 2.52 s, sys: 678 ms, total: 3.2 s\n",
      "Wall time: 8.06 s\n",
      "CPU times: user 2.87 s, sys: 630 ms, total: 3.5 s\n",
      "Wall time: 9.72 s\n",
      "CPU times: user 2.22 s, sys: 540 ms, total: 2.76 s\n",
      "Wall time: 6.56 s\n",
      "CPU times: user 3.03 s, sys: 648 ms, total: 3.68 s\n",
      "Wall time: 9.97 s\n",
      "CPU times: user 2.75 s, sys: 618 ms, total: 3.37 s\n",
      "Wall time: 8.48 s\n",
      "CPU times: user 2.8 s, sys: 642 ms, total: 3.44 s\n",
      "Wall time: 10.2 s\n",
      "CPU times: user 2.49 s, sys: 539 ms, total: 3.03 s\n",
      "Wall time: 8.24 s\n",
      "CPU times: user 3.15 s, sys: 774 ms, total: 3.93 s\n",
      "Wall time: 10.9 s\n",
      "CPU times: user 2.47 s, sys: 658 ms, total: 3.12 s\n",
      "Wall time: 8.14 s\n",
      "CPU times: user 2.89 s, sys: 689 ms, total: 3.58 s\n",
      "Wall time: 10.4 s\n",
      "CPU times: user 2.49 s, sys: 499 ms, total: 2.99 s\n",
      "Wall time: 8.11 s\n",
      "CPU times: user 2.85 s, sys: 795 ms, total: 3.64 s\n",
      "Wall time: 9.27 s\n",
      "CPU times: user 2.08 s, sys: 409 ms, total: 2.49 s\n",
      "Wall time: 7.96 s\n",
      "CPU times: user 2.55 s, sys: 609 ms, total: 3.16 s\n",
      "Wall time: 7.63 s\n",
      "CPU times: user 2.33 s, sys: 510 ms, total: 2.84 s\n",
      "Wall time: 8.24 s\n",
      "CPU times: user 2.77 s, sys: 572 ms, total: 3.34 s\n",
      "Wall time: 9.21 s\n",
      "CPU times: user 1.58 s, sys: 400 ms, total: 1.98 s\n",
      "Wall time: 6.58 s\n",
      "CPU times: user 1.99 s, sys: 458 ms, total: 2.45 s\n",
      "Wall time: 7.98 s\n"
     ]
    }
   ],
   "source": [
    "# Quicker initial run to test performance\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"lda_training.log\",\n",
    "                    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.WARNING) # Using WARNING level initially\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def train_and_save_lda(corpus_tfidf, dictionary, num_topics, alpha, eta, passes, filename, keywords_lemmatized):\n",
    "    # Set the logging level to INFO inside this function\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    lda_model = LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary, \n",
    "                             passes=passes, iterations=50, alpha=alpha, eta=eta, workers=2)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    # Save the model\n",
    "    lda_model.save(filename + \".lda\")\n",
    "    \n",
    "    # Log the coherence value\n",
    "    logger.info(f\"Saved {filename} with coherence {coherence_lda}\")\n",
    "\n",
    "    # Reset the logging level to WARNING after function completion\n",
    "    logger.setLevel(logging.WARNING)\n",
    "    \n",
    "    return coherence_lda\n",
    "\n",
    "# Define parameters range\n",
    "num_topics_list = [5, 10, 20]\n",
    "alpha_list = ['symmetric', 0.1, 0.9] # Reduced to 3 values for quicker sweep\n",
    "eta_list = ['auto', 'symmetric', 0.1, 0.9] # Reduced to 4 values for quicker sweep\n",
    "passes_list = [10, 15] # Reduced for early stopping\n",
    "\n",
    "# Iterate and train\n",
    "for num_topics in num_topics_list:\n",
    "    for alpha in alpha_list:\n",
    "        for eta in eta_list:\n",
    "            for passes in passes_list:\n",
    "                filename = f\"lda_nt{num_topics}_alpha{alpha}_eta{eta}_passes{passes}\"\n",
    "                %time train_and_save_lda(corpus_tfidf, dictionary, num_topics, alpha, eta, passes, filename, keywords_lemmatized) # Profiling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77797fe0-f214-417a-9117-657323af0307",
   "metadata": {},
   "source": [
    "# Finetuned run. Run with results from previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5534ef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting term detection to prioritize bi-grams and tri-grams\n",
    "# Cell after was the initial attempt\n",
    "\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# parts of speech tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    tag = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag.get(treebank_tag[0], wordnet.NOUN)\n",
    "\n",
    "# lemmatizing tokens and cleaning data\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Remove special characters while keeping possessive apostrophes\n",
    "    sentence = re.sub(r\"[^a-zA-Z0-9_' ]\", \"\", sentence)\n",
    "    tokens = word_tokenize(sentence)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in nltk.pos_tag(tokens)]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# In case lemmatization and stopwords missed things plus more weight on bi-grams and tri-grams\n",
    "def preprocess_keywords(data):\n",
    "    keywords = data['terms'].str.lower().str.split().tolist()  # Convert to lowercase\n",
    "    keywords = [[word for word in keyword if word not in gensim.parsing.preprocessing.STOPWORDS and len(word) > 2 and word.isalnum()] for keyword in keywords]\n",
    "    \n",
    "    # Better bi-grams\n",
    "    bigram_transformer = Phrases(keywords, min_count=3, threshold=50)  \n",
    "    keywords_bigram = [bigram_transformer[keyword] for keyword in keywords]\n",
    "    \n",
    "    # Better tri-gram\n",
    "    trigram_transformer = Phrases(keywords_bigram, min_count=3, threshold=50)  \n",
    "    keywords_trigram = [trigram_transformer[keyword] for keyword in keywords_bigram]\n",
    "    \n",
    "    return [lemmatize_sentence(' '.join(keyword)) for keyword in keywords_trigram]\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"corrected_data_symspell.csv\")\n",
    "\n",
    "\n",
    "# Preprocess the data to get lemmatized keywords\n",
    "keywords_lemmatized = preprocess_keywords(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and corpus for LDA\n",
    "dictionary = corpora.Dictionary(keywords_lemmatized)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "corpus = [dictionary.doc2bow(term) for term in keywords_lemmatized]\n",
    "\n",
    "# TF-IDF Weighting\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a9f43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over 5-20 topics, with varying alpha and beta, and 10-25 passes. Including logging\n",
    "import logging\n",
    "# Setup logging\n",
    "logging.basicConfig(filename=\"lda_training.log\",\n",
    "                    format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def train_and_save_lda(corpus_tfidf, dictionary, num_topics, alpha, eta, passes, filename, keywords_lemmatized):\n",
    "    lda_model = LdaMulticore(corpus_tfidf, num_topics=num_topics, id2word=dictionary, \n",
    "                             passes=passes, iterations=100, alpha=alpha, eta=eta, workers=12)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    # Save the model\n",
    "    lda_model.save(filename + \".lda\")\n",
    "    \n",
    "    # Log the coherence value\n",
    "    logger.info(f\"Saved {filename} with coherence {coherence_lda}\")\n",
    "    \n",
    "    return coherence_lda\n",
    "\n",
    "# Define parameters range\n",
    "num_topics_list = [5, 10, 20]\n",
    "alpha_list = ['symmetric', 0.1, 0.3, 0.9]\n",
    "eta_list = ['auto', 'asymmetric', 0.1, 0.3, 0.9]\n",
    "passes_list = [10, 15, 20]\n",
    "\n",
    "# Iterate and train\n",
    "for num_topics in num_topics_list:\n",
    "    for alpha in alpha_list:\n",
    "        for eta in eta_list:\n",
    "            for passes in passes_list:\n",
    "                filename = f\"lda_nt{num_topics}_alpha{alpha}_eta{eta}_passes{passes}\"\n",
    "                train_and_save_lda(corpus_tfidf, dictionary, num_topics, alpha, eta, passes, filename, keywords_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with highest coherence\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# Load the best LDA model from disk\n",
    "best_lda_model = LdaMulticore.load(\"lda_nt15_alpha0.1_etasymmetric_passes20.lda\")\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data = gensimvis.prepare(best_lda_model, corpus, dictionary)\n",
    "\n",
    "# Visualize\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac2a0f",
   "metadata": {},
   "source": [
    "## Iterative Training for HDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_hdp(corpus, dictionary, gamma, alpha, kappa, tau, filename, keywords_lemmatized):\n",
    "    hdp_model = HdpModel(corpus, id2word=dictionary, gamma=gamma, alpha=alpha, kappa=kappa, tau=tau)\n",
    "    \n",
    "    # Compute Coherence Score\n",
    "    coherence_model_hdp = CoherenceModel(model=hdp_model, texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_hdp = coherence_model_hdp.get_coherence()\n",
    "    hdp_model.save(filename + \".hdp\")\n",
    "    # return the coherence for logging purposes\n",
    "    return coherence_hdp\n",
    "\n",
    "# Define your parameters range\n",
    "gamma_list = [0.5, 1.0, 1.5]\n",
    "alpha_list = [0.5, 1.0, 1.5]\n",
    "kappa_list = [0.5, 1.0, 1.5]\n",
    "tau_list = [32.0, 64.0]\n",
    "\n",
    "# Iterate and train\n",
    "for gamma in gamma_list:\n",
    "    for alpha in alpha_list:\n",
    "        for kappa in kappa_list:\n",
    "            for tau in tau_list:\n",
    "                filename = f\"hdp_gamma{gamma}_alpha{alpha}_kappa{kappa}_tau{tau}\"\n",
    "                coherence = train_and_save_hdp(corpus, dictionary, gamma, alpha, kappa, tau, filename, keywords_lemmatized)\n",
    "                print(f\"Saved {filename} with coherence {coherence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import HdpModel\n",
    "\n",
    "hdp_model = HdpModel.load(\"hdp_gamma1.5_alpha1.0_kappa1.0_tau32.0.hdp\")\n",
    "\n",
    "lda_model = hdp_model.suggested_lda_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc0412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c090486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5d159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6286a585",
   "metadata": {},
   "source": [
    "Exporting LDA Topics and Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe75706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Number of topics and terms\n",
    "num_topics = 15\n",
    "num_words = 20\n",
    "\n",
    "# Extract the topics from the model\n",
    "topics = best_lda_model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "\n",
    "# Extract topics and their coherence\n",
    "topic_terms = []\n",
    "topic_coherences = []\n",
    "for topic_num, terms in topics:\n",
    "    topic = [word for word, _ in terms]\n",
    "    topic_terms.append(topic)\n",
    "    \n",
    "    # Compute coherence for each topic\n",
    "    cm = CoherenceModel(topics=[topic], texts=keywords_lemmatized, dictionary=dictionary, coherence='c_v')\n",
    "    coherence = cm.get_coherence()\n",
    "    topic_coherences.append(coherence)\n",
    "\n",
    "# Create a DataFrame to store the topics, terms, and coherence\n",
    "df = pd.DataFrame({\n",
    "    'Topic_Num': range(1, num_topics + 1),\n",
    "    'Topic_Terms': [' '.join(terms) for terms in topic_terms],\n",
    "    'Coherence': topic_coherences\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"topics_and_coherence.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04188c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
